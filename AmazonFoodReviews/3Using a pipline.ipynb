{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a ScikitLearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all relevent libraries to analyse the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "## This line makes sure that our graphs are rendered within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of a pipeline is to bring all the pieces of the model building together in one place so that in can be take into further anaysis or plugged in to another workflow or app. A data engineer can then take it and use some wrapper to put all these components as a plug into another process. \n",
    "\n",
    "The first step is to bring all the functions created in the data preparation part and then finally putting all together one single process that includes a pipeline that calls them all. The follwoing cells will take you through this procedures. \n",
    "\n",
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative and positive rating flags\n",
    "def get_rating_flags(score):\n",
    "    def negative_rating(score):\n",
    "        if score < 3:\n",
    "            return '1'\n",
    "        return '0'\n",
    "    def positive_rating(score):\n",
    "        if score > 3:\n",
    "            return '1'\n",
    "        return '0'\n",
    "    negative = negative_rating(score)\n",
    "    positive = positive_rating(score)\n",
    "    return [negative, positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting word counts\n",
    "def get_word_counts(text):\n",
    "    tokenized_word=tokenizer.tokenize(text)\n",
    "    fdist = FreqDist(tokenized_word)\n",
    "    return [fdist.B(), fdist.N()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winter and summer flags\n",
    "def get_season_flags(month):\n",
    "    def winter_flag(month):\n",
    "        if month in [10, 11, 12 , 1, 2]:\n",
    "            return '1'\n",
    "        return '0'\n",
    "    def summer_flag(month):\n",
    "        if month in [7, 8]:\n",
    "            return '1'\n",
    "        return '0'\n",
    "    winter = winter_flag(month)\n",
    "    summer = summer_flag(month)\n",
    "    return [winter, summer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function in charge of calling the functions created above and other data changes\n",
    "def new_features(data):\n",
    "    new_df = data.copy()\n",
    "    #Score variables\n",
    "    rating_flags = new_df.apply(\n",
    "        lambda x: pd.Series(\n",
    "            get_rating_flags(x.Score), \n",
    "            index = [\"negative_rating_flag\", \"positive_rating_flag\"]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    new_df = pd.concat([new_df[:], rating_flags[:]], axis=1)\n",
    "    \n",
    "    #Text variables\n",
    "    text_attributes = new_df.apply(\n",
    "        lambda x: pd.Series(\n",
    "            get_word_counts(x.Text), \n",
    "            index = [\"n_distinct_words\", \"n_words\"]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    new_df = pd.concat([new_df[:], text_attributes[:]], axis=1)\n",
    "    \n",
    "    #Time variables\n",
    "    new_df['Time'] = pd.to_datetime(new_df['Time'], unit='s')\n",
    "    new_df['day_of_week'] = new_df['Time'].dt.weekday\n",
    "    new_df['month'] = new_df['Time'].dt.month\n",
    "\n",
    "    season_flags = new_df.apply(\n",
    "        lambda x: pd.Series(\n",
    "            get_season_flags(x.month), \n",
    "            index = [\"winter_flag\", \"summer_flag\"]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    new_df = pd.concat([new_df[:], season_flags[:]], axis=1)\n",
    "    \n",
    "    #Product and Reviewer variables\n",
    "    new_df['product_freq'] = new_df.groupby('ProductId')['ProductId'].transform('count')\n",
    "    new_df['reviewer_freq'] = new_df.groupby('UserId')['UserId'].transform('count')\n",
    "    \n",
    "    model_df = new_df[['Score'\n",
    "                        ,'negative_rating_flag'\n",
    "                        ,'positive_rating_flag'\n",
    "                        ,'day_of_week'\n",
    "                        ,'month'\n",
    "                        ,'winter_flag'\n",
    "                        ,'summer_flag'\n",
    "                        ,'n_words'\n",
    "                        ,'product_freq'\n",
    "                        ,'reviewer_freq']]\n",
    "\n",
    "    return model_df\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling all preparations and models with a scikit learn pipeline\n",
    "\n",
    "Once all the functions are in place then the followwing code tides everything together:\n",
    "1. Reads the raw data\n",
    "1. Defines the data set, in this case all reviews with helpful denominator > 4\n",
    "1. Call the data preparation function through the Function Transformer called dataprep_transformer\n",
    "1. Modify the data further by creating dummy variables for the categorical variables and standarising for the numerical ones through a Mapper. \n",
    "1. Calling all the previous steps in a pipeline and adding the Logistic regression module\n",
    "1. Fitting the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('Reviews.csv')\n",
    "\n",
    "# Defining the data set\n",
    "DenominatorBiggerthan3_data = raw_data[raw_data['HelpfulnessDenominator']>4]\n",
    "\n",
    "#Defining the target\n",
    "def calculate_helpfulness_ratio(Numerator, Denominator):\n",
    "    ratio = Numerator/float(Denominator)\n",
    "    def target(ratio): \n",
    "        if ratio > 0.7: \n",
    "            return '1'\n",
    "        return '0'\n",
    "    useful_flag = target(ratio)\n",
    "    #ratio = round(ratio,2)\n",
    "    return [ratio, useful_flag]\n",
    "\n",
    "helpfulness_ratio = DenominatorBiggerthan3_data.apply(\n",
    "    lambda x: pd.Series(\n",
    "        calculate_helpfulness_ratio(x.HelpfulnessNumerator, x.HelpfulnessDenominator), \n",
    "        index = [\"helpfulness_ratio\", \"useful_flag\"]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "HelpfulnesswithTarget_df = pd.concat([DenominatorBiggerthan3_data[:], helpfulness_ratio[:]], axis=1)\n",
    "\n",
    "dataprep_transformer = FunctionTransformer(new_features, validate=False)\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    ('day_of_week', LabelBinarizer()),\n",
    "    ('month', LabelBinarizer()),\n",
    "    (['n_words'], StandardScaler()),\n",
    "    (['product_freq'], StandardScaler()),\n",
    "    (['reviewer_freq'], StandardScaler()),\n",
    "    (['Score'], StandardScaler())\n",
    "], default=None)\n",
    "\n",
    "pipe = sklearn.pipeline.Pipeline([\n",
    "    ('dataprep', dataprep_transformer),   \n",
    "    ('featurize', mapper),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "model = pipe.fit(\n",
    "    X=HelpfulnesswithTarget_df, \n",
    "    y=HelpfulnesswithTarget_df.useful_flag.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('day_of_week_0', 0.037339849694803376),\n",
       " ('day_of_week_1', -0.015561571195092359),\n",
       " ('day_of_week_2', 0.03531995804717539),\n",
       " ('day_of_week_3', -0.0315097126385658),\n",
       " ('day_of_week_4', 0.035748546663204143),\n",
       " ('day_of_week_5', 0.018097284214518766),\n",
       " ('day_of_week_6', 0.013389343790562391),\n",
       " ('month_1', 0.03045793616216519),\n",
       " ('month_2', 0.03294235144722164),\n",
       " ('month_3', 0.008589147856367416),\n",
       " ('month_4', -0.03147642030008585),\n",
       " ('month_5', -0.08654927060487967),\n",
       " ('month_6', 0.003075582847269026),\n",
       " ('month_7', 0.0019249979152510851),\n",
       " ('month_8', 0.07582113766184309),\n",
       " ('month_9', -0.003732301817545515),\n",
       " ('month_10', 0.0015949850669462365),\n",
       " ('month_11', -0.0009324543822984366),\n",
       " ('month_12', 0.061108006724264745),\n",
       " ('n_words', 0.30987650244242254),\n",
       " ('product_freq', -0.1702660201771724),\n",
       " ('reviewer_freq', -0.06803649921727592),\n",
       " ('Score', 0.6951813581961932),\n",
       " ('negative_rating_flag', 0.23687678233091417),\n",
       " ('positive_rating_flag', 0.8913147477523748),\n",
       " ('winter_flag', 0.1251708250185578),\n",
       " ('summer_flag', 0.07774613557716663)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(\n",
    "    model.named_steps['featurize'].transformed_names_, \n",
    "    model.named_steps[\"log_reg\"].coef_[0]\n",
    ")\n",
    "logreg = model.named_steps[\"log_reg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this all runs smoothly it means we can run the model again and see the coeficients at the end. It also means that the steps are all together in one place and if there are any data updates then the only part to modify is exactly this, the raw data (assuming the column names don't change). There are also other parts that can be added like feature selection or cross validation to mention some.\n",
    "\n",
    "I think this is such a cool way to put all the statistical efforts together in one place :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
