{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sckit-Learn - Estimators APIs\n",
    "\n",
    "In this section the data from the data manipulation and feature engineering section will be imported and manipulated one more time in order to run a Logistic regression classifier. The possibility of predicting whether a review is useful or not will be assessed and the model's performance will be measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all relevent libraries to analyse the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "## This line makes sure that our graphs are rendered within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "## Set options to display all columns\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "# load\n",
    "HelpfulnesswithTarget_df = pd.read_csv('HelpfulnesswithTarget_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scikit learn libraries and APIs needed\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                           int64\n",
      "ProductId                   object\n",
      "UserId                      object\n",
      "ProfileName                 object\n",
      "HelpfulnessNumerator         int64\n",
      "HelpfulnessDenominator       int64\n",
      "Score                        int64\n",
      "Time                        object\n",
      "Summary                     object\n",
      "Text                        object\n",
      "helpfulness_ratio          float64\n",
      "useful_flag                  int64\n",
      "negative_rating_flag         int64\n",
      "positive_rating_flag         int64\n",
      "n_distinct_words             int64\n",
      "n_words                      int64\n",
      "clean_text                  object\n",
      "n_distinct_cleanwords        int64\n",
      "n_cleanwords                 int64\n",
      "cleanwords_ratio           float64\n",
      "day_of_week               category\n",
      "year                         int64\n",
      "month                     category\n",
      "product_freq                 int64\n",
      "reviewer_freq                int64\n",
      "winter_flag                  int64\n",
      "summer_flag                  int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(HelpfulnesswithTarget_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First que split the data into a features matrix (X) and target vector (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting data type for categorical variables\n",
    "HelpfulnesswithTarget_df['day_of_week'] = HelpfulnesswithTarget_df['day_of_week'].astype('category') \n",
    "HelpfulnesswithTarget_df['month'] = HelpfulnesswithTarget_df['month'].astype('category') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = HelpfulnesswithTarget_df[['Score'\n",
    "                                   ,'negative_rating_flag'\n",
    "                                   ,'positive_rating_flag'\n",
    "                                   ,'day_of_week'\n",
    "                                   ,'month'\n",
    "                                   ,'winter_flag'\n",
    "                                   ,'summer_flag'\n",
    "                                   ,'n_words'\n",
    "                                   ,'product_freq'\n",
    "                                   ,'reviewer_freq']]\n",
    "\n",
    "target = HelpfulnesswithTarget_df[['useful_flag']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we turn each of the categorical variables levels into a bianry variable. Leaving one of the catergies out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>negative_rating_flag</th>\n",
       "      <th>positive_rating_flag</th>\n",
       "      <th>winter_flag</th>\n",
       "      <th>summer_flag</th>\n",
       "      <th>n_words</th>\n",
       "      <th>product_freq</th>\n",
       "      <th>reviewer_freq</th>\n",
       "      <th>day_of_week_1</th>\n",
       "      <th>day_of_week_2</th>\n",
       "      <th>day_of_week_3</th>\n",
       "      <th>day_of_week_4</th>\n",
       "      <th>day_of_week_5</th>\n",
       "      <th>day_of_week_6</th>\n",
       "      <th>month_2</th>\n",
       "      <th>month_3</th>\n",
       "      <th>month_4</th>\n",
       "      <th>month_5</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Score  negative_rating_flag  positive_rating_flag  winter_flag  \\\n",
       "3       2                     1                     0            0   \n",
       "11      5                     0                     1            0   \n",
       "14      5                     0                     1            0   \n",
       "15      5                     0                     1            1   \n",
       "32      4                     0                     1            1   \n",
       "\n",
       "    summer_flag  n_words  product_freq  reviewer_freq  day_of_week_1  \\\n",
       "3             0       41             1              3              0   \n",
       "11            1       67             1              3              0   \n",
       "14            0       21             2             42              0   \n",
       "15            0       26             2              1              1   \n",
       "32            0      206             5              6              0   \n",
       "\n",
       "    day_of_week_2  day_of_week_3  day_of_week_4  day_of_week_5  day_of_week_6  \\\n",
       "3               0              0              0              0              0   \n",
       "11              0              0              1              0              0   \n",
       "14              0              0              1              0              0   \n",
       "15              0              0              0              0              0   \n",
       "32              0              0              0              0              0   \n",
       "\n",
       "    month_2  month_3  month_4  month_5  month_6  month_7  month_8  month_9  \\\n",
       "3         0        0        0        0        1        0        0        0   \n",
       "11        0        0        0        0        0        0        1        0   \n",
       "14        0        1        0        0        0        0        0        0   \n",
       "15        0        0        0        0        0        0        0        0   \n",
       "32        0        0        0        0        0        0        0        0   \n",
       "\n",
       "    month_10  month_11  month_12  \n",
       "3          0         0         0  \n",
       "11         0         0         0  \n",
       "14         0         0         0  \n",
       "15         0         0         1  \n",
       "32         0         1         0  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_variables = ['day_of_week', 'month']\n",
    "for var in categorical_variables: # After the first look of the results I would put this in a function\n",
    "    categorical_list='var'+'_'+var\n",
    "    categorical_list = pd.get_dummies(model_df[var], prefix=var, drop_first=True)\n",
    "    data1=model_df.join(categorical_list)\n",
    "    model_df=data1\n",
    "    \n",
    "data_vars=model_df.columns.values.tolist()\n",
    "to_keep=[i for i in data_vars if i not in categorical_variables]\n",
    "\n",
    "model_df=model_df[to_keep]\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Preparing train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split our data into train and tests sets to measure and compare model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124167, 25), (99333, 25), (24834, 25))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(model_df, target, test_size=0.2, random_state=42)\n",
    "model_df.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known from the data exploration section that our target has more reviews tagged as useful (1) and this can bring problems to the model's results and interpretation.  One way to come around this problem is to over sample the not useful reviews, so that the resulted data set has a balanced split of target observations. This will only be done on the training data set as we want to test if the coefficients will give us an accurate prediction on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x18f88d90>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(x=\"useful_flag\", kind=\"count\", data = HelpfulnesswithTarget_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('length of oversampled data is ', 131418)\n",
      "('Number of useful_flag = 0', 65709)\n",
      "('Number of useful_flag = 1', 65709)\n"
     ]
    }
   ],
   "source": [
    "# Sampling up none useful reviews using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "os = SMOTE(random_state=0)\n",
    "os_data_X,os_data_y=os.fit_sample(X_train, y_train)\n",
    "os_data_X = pd.DataFrame(data=os_data_X, columns=X_train.columns )\n",
    "os_data_y= pd.DataFrame(data=os_data_y,columns=['useful_flag'])\n",
    "\n",
    "# Check data dimensions\n",
    "print('length of oversampled data is ',len(os_data_X))\n",
    "print('Number of useful_flag = 0',len(os_data_y[os_data_y['useful_flag']==0]))\n",
    "print('Number of useful_flag = 1',len(os_data_y[os_data_y['useful_flag']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification modelling Logistic regression\n",
    "Logistic regression is a classification technique, where the output variable is either binary or a multiclass (categorical variables with different levels, like segments for example) compared to linear regression where the output is a numerical value. \n",
    "\n",
    "The main model assumptions are:\n",
    "- The independent variables should be independent of each other. That is, the model should have little or no multicollinearity.\n",
    "- The independent variables are linearly related to the output.\n",
    "- Logistic regression requires quite large sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable selection\n",
    "First we will assess which variables we can take out of the model by looking at the model summary table and comparing the p-values with an alpha =0.05 (which is I am assuming is the default) The p-values greater than this will indicate the variable should be removed from the model's equation. \n",
    "In practice, there are different techniques to select the variables that give the best model performance. Some of these techniques are: Forward selection, backward selection and stepwise selection. It is also important to consider the business or study context. Is the combination of these that gives the best variable selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70901609, -0.23180117, -1.04000497],\n",
       "       [-0.70901609, -0.23180117,  0.80941186],\n",
       "       [-0.67365471,  2.48251539,  0.80941186],\n",
       "       ...,\n",
       "       [-0.60293195, -0.37099689,  0.80941186],\n",
       "       [-0.60293195, -0.37099689,  0.19293958],\n",
       "       [-0.70901609, -0.37099689,  0.80941186]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#should be used to standarise the continuous variables\n",
    "#StandardScaler().fit_transform(model_df[['product_freq','reviewer_freq','Score']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False False\n",
      "  True]\n",
      "[1 1 1 1 1 6 4 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Recursive feature elimination\n",
    "data_final_vars=model_df.columns.values.tolist()\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFE(logreg, 20)\n",
    "rfe = rfe.fit(os_data_X, os_data_y.values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Scikit learn documentation:\n",
    "support : array\n",
    "An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True if its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**statsmodels** is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. (https://www.statsmodels.org/stable/index.html)\n",
    "This module is useful for checking summary tables and interpreting the model's results, I also like it because it resembles the R output and I am familiar with that. From what I have read, Scikit-Learn does not provide this kind of outputs since it focuses more on the machine learning questions and processes, while Statsmodels focus more on the statistical results. The latter is useful to understand what is happening underneath whilst assesing performance and results; plus it is also needed for interpretation purposes. \n",
    "\n",
    "In practice, once I understand a model and its outputs I tend to find a way of optimising the best result by using loops and/or functions via grid searches and arrays using R. In Python the best way will be to use pipelines which will also mean I can talk to the engineering side of the business when talking about implementation and model production. These kind of workflows will output different solutions from where then I can pick the best. The best being the one that gives me the best performance and easy interpretation when taking to my stakeholders; plus it means the model doesn not have to be built from scratch every time and it can be plugged in to some other interface if required. Notebook 3 shows how this model could be implemented in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.535667\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\statsmodels\\base\\model.py:512: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Results: Logit\n",
      "==================================================================================\n",
      "Model:                    Logit                 Pseudo R-squared:      0.227      \n",
      "Dependent Variable:       useful_flag           AIC:                   140842.4976\n",
      "Date:                     2019-06-28 10:58      BIC:                   141087.1511\n",
      "No. Observations:         131418                Log-Likelihood:        -70396.    \n",
      "Df Model:                 24                    LL-Null:               -91092.    \n",
      "Df Residuals:             131393                LLR p-value:           0.0000     \n",
      "Converged:                0.0000                Scale:                 1.0000     \n",
      "No. Iterations:           35.0000                                                 \n",
      "----------------------------------------------------------------------------------\n",
      "                      Coef.    Std.Err.      z     P>|z|     [0.025       0.975]  \n",
      "----------------------------------------------------------------------------------\n",
      "Score                 -0.8064     0.0103  -78.1400 0.0000      -0.8266     -0.7861\n",
      "negative_rating_flag  -2.3389     0.0228 -102.4647 0.0000      -2.3837     -2.2942\n",
      "positive_rating_flag   2.7324     0.0329   82.9299 0.0000       2.6679      2.7970\n",
      "winter_flag            1.3006     0.0324   40.1575 0.0000       1.2372      1.3641\n",
      "summer_flag          -26.5305 59309.4940   -0.0004 0.9996 -116271.0026 116217.9416\n",
      "n_words                0.0020     0.0001   29.2669 0.0000       0.0018      0.0021\n",
      "product_freq          -0.0087     0.0002  -38.0269 0.0000      -0.0091     -0.0082\n",
      "reviewer_freq         -0.0069     0.0005  -14.9324 0.0000      -0.0078     -0.0060\n",
      "day_of_week_1          0.5758     0.0228   25.2510 0.0000       0.5311      0.6205\n",
      "day_of_week_2          0.6154     0.0227   27.0681 0.0000       0.5709      0.6600\n",
      "day_of_week_3          0.5451     0.0228   23.9138 0.0000       0.5004      0.5897\n",
      "day_of_week_4          0.6045     0.0233   25.9413 0.0000       0.5588      0.6502\n",
      "day_of_week_5          0.6724     0.0240   27.9718 0.0000       0.6253      0.7195\n",
      "day_of_week_6          0.6180     0.0240   25.7414 0.0000       0.5710      0.6651\n",
      "month_2                0.6500     0.0296   21.9596 0.0000       0.5920      0.7081\n",
      "month_3                1.7802     0.0358   49.7183 0.0000       1.7100      1.8504\n",
      "month_4                1.7745     0.0371   47.8066 0.0000       1.7018      1.8473\n",
      "month_5                1.6673     0.0363   45.9213 0.0000       1.5961      1.7384\n",
      "month_6                1.8138     0.0379   47.9006 0.0000       1.7396      1.8880\n",
      "month_7               28.4060 59309.4940    0.0005 0.9996 -116216.0661 116272.8781\n",
      "month_8               28.4544 59309.4940    0.0005 0.9996 -116216.0177 116272.9265\n",
      "month_9                1.7120     0.0369   46.4226 0.0000       1.6397      1.7843\n",
      "month_10               0.6073     0.0303   20.0365 0.0000       0.5479      0.6667\n",
      "month_11               0.5967     0.0308   19.3664 0.0000       0.5363      0.6571\n",
      "month_12               0.6618     0.0298   22.2283 0.0000       0.6035      0.7202\n",
      "==================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "logit_model=sm.Logit(os_data_y,os_data_X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table suggests that some season flags and some months variables to be removed from data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would be better to have this on a function so we see the table just once. \n",
    "# columns_notselected=['winter_flag','summer_flag','month_1','month_2','month_4'\n",
    "#                      ,'month_7','month_8','month_9','month_10','month_11','month_12'] #without using drop-first\n",
    "columns_notselected=['summer_flag', 'month_7', 'month_8'] #using drop first\n",
    "\n",
    "columns_OSdata=os_data_X.columns.values.tolist()\n",
    "\n",
    "columns_tokeep=[i for i in columns_OSdata if i not in columns_notselected]\n",
    "os_data_Xselected=os_data_X[columns_tokeep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After taking those variables out, we have a look at the table again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.551133\n",
      "         Iterations 6\n",
      "                           Results: Logit\n",
      "=====================================================================\n",
      "Model:               Logit             Pseudo R-squared:  0.205      \n",
      "Dependent Variable:  useful_flag       AIC:               144901.5350\n",
      "Date:                2019-06-28 10:59  BIC:               145116.8301\n",
      "No. Observations:    131418            Log-Likelihood:    -72429.    \n",
      "Df Model:            21                LL-Null:           -91092.    \n",
      "Df Residuals:        131396            LLR p-value:       0.0000     \n",
      "Converged:           1.0000            Scale:             1.0000     \n",
      "No. Iterations:      6.0000                                          \n",
      "---------------------------------------------------------------------\n",
      "                      Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "---------------------------------------------------------------------\n",
      "Score                -0.4967   0.0077 -64.2903 0.0000 -0.5118 -0.4816\n",
      "negative_rating_flag -1.6660   0.0178 -93.6466 0.0000 -1.7008 -1.6311\n",
      "positive_rating_flag  2.2967   0.0304  75.6085 0.0000  2.2372  2.3562\n",
      "winter_flag           0.0808   0.0224   3.6136 0.0003  0.0370  0.1247\n",
      "n_words               0.0020   0.0001  30.0421 0.0000  0.0018  0.0021\n",
      "product_freq         -0.0083   0.0002 -36.9561 0.0000 -0.0088 -0.0079\n",
      "reviewer_freq        -0.0056   0.0005 -12.2697 0.0000 -0.0065 -0.0047\n",
      "day_of_week_1         0.8310   0.0221  37.5533 0.0000  0.7876  0.8744\n",
      "day_of_week_2         0.8704   0.0220  39.4985 0.0000  0.8272  0.9136\n",
      "day_of_week_3         0.7915   0.0221  35.7665 0.0000  0.7481  0.8349\n",
      "day_of_week_4         0.8640   0.0226  38.1488 0.0000  0.8196  0.9084\n",
      "day_of_week_5         0.9267   0.0234  39.5945 0.0000  0.8808  0.9725\n",
      "day_of_week_6         0.8807   0.0233  37.7268 0.0000  0.8350  0.9265\n",
      "month_2               0.6023   0.0297  20.2588 0.0000  0.5441  0.6606\n",
      "month_3               0.5166   0.0263  19.6066 0.0000  0.4649  0.5682\n",
      "month_4               0.5056   0.0281  18.0195 0.0000  0.4506  0.5606\n",
      "month_5               0.3936   0.0269  14.6224 0.0000  0.3409  0.4464\n",
      "month_6               0.5419   0.0290  18.7017 0.0000  0.4851  0.5987\n",
      "month_9               0.4410   0.0277  15.9174 0.0000  0.3867  0.4953\n",
      "month_10              0.5578   0.0305  18.3168 0.0000  0.4981  0.6175\n",
      "month_11              0.5507   0.0309  17.7994 0.0000  0.4901  0.6114\n",
      "month_12              0.6012   0.0299  20.1137 0.0000  0.5426  0.6598\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit_model=sm.Logit(os_data_y,os_data_Xselected)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, all the variables are significant which means there is no need to take any out anymore. However, it is important to have a model that can be interpreted as well. Model performance and robustness is just as important as interpretation. This last one can be a bit daunting in general for logistic regression due to the nature of the variables. \n",
    "\n",
    "The value of the coefficients from 'day of the week' variables are all very similar; meaning there is not a lot of difference between them (which ties up with the data exploration section). It is almost the same case for the months, except from May and the winter months December to February. A review made on the winter months contributes to the useful prediction slightly more than if a review is made in any other month. For example for December the coefficient is 0.6 and for May is 0.39, meaning that if everything else stays the same, a review made on December adds 0.6 to the log of the odds ratio compared to May where it will contirbute 0.39. Odds ratio being the ratio of the review being useful if posted on a specific month. \n",
    "Or in simpler terms, if we assume everything else remains the same, reviews made in December and February increase the chances of the review being useful compared to reviews made on May. Which also ties up with what it was shown on the exploration part. \n",
    "\n",
    "This can also be extended to the variables related to score. These are shown to be important in predicting whether a review is useful or not, which also makes sense with the data exploration section. The more a reviewer is disatisfied with the product the less useful the review is. \n",
    "\n",
    "It would still be useful to have more text mining variables associated with the review to have more information about the reviews itself and it might be the case that the actual content will be assesed better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performance\n",
    "\n",
    "~ Better that ten guilty persons escape than one innocent suffers ~ \n",
    "\n",
    "Once the variables that are important are selected and the coefficients are calculated, then the model is fitted on the test set to get the prediction values of th target. Which will allow to compare the predicted results(y's) against the real resutls in order to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model on train set\n",
    "logreg.fit(os_data_Xselected, os_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy on train set is: 0.755\n"
     ]
    }
   ],
   "source": [
    "print('The model accuracy on train set is: {:.3f}'.format(logreg.score(os_data_Xselected, os_data_y))) \n",
    "#without using drop-first accuracy was 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.74\n",
      "Confusion matrix:\n",
      "[[ 5662  2785]\n",
      " [ 3714 12673]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.67      0.64      8447\n",
      "           1       0.82      0.77      0.80     16387\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     24834\n",
      "   macro avg       0.71      0.72      0.72     24834\n",
      "weighted avg       0.75      0.74      0.74     24834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicting values in test set\n",
    "from sklearn import metrics\n",
    "\n",
    "# selecting only the columns needed in the test set\n",
    "Xselected_test=X_test[columns_tokeep]\n",
    "\n",
    "y_pred = logreg.predict(Xselected_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(Xselected_test, y_test)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24834"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xselected_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, the model got 18,335 (5662+12673) predictions right, and 6,499 wrong from a total of 24,834 observations (reviews) on the test set. Which is where the 74% accuracy comes from. \n",
    "\n",
    "Looking at precision and recall, 82% of the model's positives and 60% of the negatives were correctly predicted (precision); whilst 77% of the positives observation and 67% of the negatives were predicted correctly. These two metrics are a bit confusing but in general:\n",
    "- **precision** assesses how many of the predictions are right\n",
    "- **recall** assesses how many observations did the model get right \n",
    "\n",
    "As an example, let's say we have 3 true positives from the model and 4 positive observations. The precision would be 3/3, i.e. all the predictions are correct; whilst the recall is 3/4, i.e. the model missed one of the true observations. \n",
    "\n",
    "The importance of these 2 measures depends on the purpose of the study or the business case. For example, in Spam detection usually precision is more important; whilst in medicine and clinical studies recall is more important. Usually these are associated with cost, and one will be more \"expensive\" than the other depending on the purpose. Hence the phrase I wrote at the beginning of the model performance section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(Xselected_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(Xselected_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is a graphical view of assessing the model's performance. The higher the AUC, better the model is at predicting 0's and 1's. If the ROC resembles the dotted redline too much then the classifier is not good at identifying the 0's and 1's. In this case, the curve does not look too bad. \n",
    "An AUC of 0.72  means there is 72% chance that model will be able to distinguish between positive class and negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# from sklearn.datasets import load_iris\n",
    "\n",
    "# data = load_iris()\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "# fig.subplots_adjust(hspace=0.5)\n",
    "# fig.suptitle('Distributions of Iris Features')\n",
    "\n",
    "# for ax, feature, name in zip(axes.flatten(), data.data.T, data.feature_names):\n",
    "#     sns.catplot( data.target_names, feature, kind=\"box\", ax=ax)\n",
    "#     #sns.distplot(feature, ax=ax, bins=len(np.unique(data.data.T[0]))//2)\n",
    "#     #sns.catplot(x=\"negative_rating_flag\", y=\"helpfulness_ratio\", kind=\"box\", data=HelpfulnesswithTarget_df,ax=ax)\n",
    "#     ax.set(title=name[:-4].upper(), xlabel='cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71769898 0.71488358 0.71572059 0.71435094 0.70917669 0.78055091\n",
      " 0.7909755  0.79173642 0.78709481 0.79550989]\n",
      "Accuracy: 0.75 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using 10-fold cross-validation\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "scores = cross_val_score(logreg, os_data_Xselected, os_data_y, scoring='accuracy', cv=10)\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a quick 10-fold cross validation algorithm, it is possible to see that this model's accuracy remains around 75% with a small confidence interval, which means it is performing well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Trying a Random Forest just out of curiousity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Fit Random Forest on training fold \n",
    "clf_rf = RandomForestClassifier(n_estimators=500, max_depth=2,\n",
    "                              random_state=0).fit(os_data_Xselected, os_data_y) #without using drop-first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = clf_rf.predict(Xselected_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Random Forest accuracy is: 0.751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('The Random Forest accuracy is: {:.3f}'.format(accuracy_score(y_test, y_model))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusions\n",
    "\n",
    "- The classifiers have a satisfactory performance and behaviour and are able to predict the reviews \n",
    "- Once one model is tested, analysed and understood then, I would put the steps into functions and a pipeline using the scikit learn API so that everything is tied up and one can just change the inital X and y elements. I would try different seeds and see which variables keep getting picked up in the model, to make sure I have tried everything.    \n",
    "- It would also be possible to test other classifiers and see which one is better or if there is any improvement. This is specilly needed for more complex models when there are millions of rows and many variables. \n",
    "- I would have probably tried to predict the usefulness ratio instead of the useful flag, as a ratio of 0.2 is not the same as 0.65 This means that there is a bit of lost information there. In this case I would have probably started with a linear regression, since the output measure is a numerical value. There is also something called a Random Forest Regression, that might be interesting to explore further.  \n",
    "- Other variables could have been added if there was more data avaiable. For example, product especifications or reviewers' details such as tenure, age or profession. \n",
    "- As mentioned before, more text mining variables could have been added as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing other solvers in logistic regression models ---\n",
    "#l1_ratio = 0.5  # L1 weight in the Elastic-Net regularization \n",
    "# this is the kind of hyperparameter that need to be found via a grid search\n",
    "\n",
    "# Fit Logistic regression on training fold\n",
    "#l1_log = LogisticRegression(random_state=0, solver='saga', penalty='l1').fit(os_data_Xselected, os_data_y)\n",
    "#l2_log = LogisticRegression(random_state=0, solver='saga', penalty='l2').fit(os_data_Xselected, os_data_y)\n",
    "#en_log = LogisticRegression(random_state=0, solver='saga', penalty='elasticnet',l1_ratio=0.5).fit(os_data_Xselected, os_data_y)\n",
    "\n",
    "# A note on elastic net: --\n",
    "# ElasticNet is a linear regression model trained with both l1 and l2-norm regularization of the coefficients. \n",
    "# This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, \n",
    "# while still maintaining the regularization properties of Ridge. We control the convex combination of l1 and l1 using \n",
    "# the l1_ratio parameter.\n",
    "\n",
    "# Elastic-net is useful when there are multiple features which are correlated with one another. \n",
    "#Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n",
    "\n",
    "# A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some \n",
    "# of Ridge’s stability under rotation.\n",
    "# so bascially, lasso (l1) works best when we have a lot of variables that are useless and Ridge (l2) works best when we have\n",
    "# a lot of variables that might not all be useless\n",
    "# when rho = 1 then elastic net is Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_model = LogisticRegression(random_state=0, solver='saga').fit(os_data_Xselected, os_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Coefficients\n",
    "# # coef_l1_log = l1_log.coef_.ravel()\n",
    "# # coef_l2_log = l2_log.coef_.ravel()\n",
    "# # print(coef_l1_log)\n",
    "# # print(coef_l2_log)\n",
    "\n",
    "# pd.DataFrame(zip(X_train.columns, np.transpose(log_model.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The model accuracy is: {:.3f}'.format(log_model.score(os_data_Xselected, os_data_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_pred = log_model.predict(Xselected_test) #predicting binary outcomes\n",
    "# log_pred_prob = log_model.predict_proba(Xselected_test) # predicting probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
