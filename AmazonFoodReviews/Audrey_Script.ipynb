{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/l0533643/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pandas import DataFrame\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "Opralog['Date']=pd.to_datetime(Opralog.ENTRY_TIMESTAMP)\n",
    "\n",
    "Opralog=Opralog[Opralog['WELLNAME'].isin(['A01','D10','D13','D14Z','D15','D19','D21','D28','D29','D35','Islay','N05','N12','N37z','N52Y','N53',\n",
    "                                                'N54Z','N56z','NGA','NGBST'])]\n",
    "\n",
    "\n",
    "\n",
    "# Opralog_pwri=Opralog[Opralog['SHADOW_COMMENT'].str.contains('PWRI', na=False)] \n",
    "# Opralog_pwri=Opralog[Opralog['SHADOW_COMMENT'].str.contains('C104', na=False)] \n",
    "\n",
    "# Opralog_pwri=Opralog[Opralog['SHADOW_COMMENT'].str.contains('stim', na=False)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/l0533643/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "string.punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stopwords.extend([\"well\", \"D14\",\"D15\",\"N55\",\"D19\",\"N54\",\"D10\",\"D13\",\"D29\",\"D28\",\"D21\",\"N56\",\"D35\",\"N12\",\"N53\",\"N05\",\n",
    "\"A01\",\"N37\",\"N51\",\"N52\",\"ISLAY\",\"NGBST\",\"NGA\",\"N50\",\"bar\",\"N56\",\"also\",\"however\"])\n",
    "exception = {'CO2','C02'}\n",
    "\n",
    "\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "#remove words that are in NLTK stopwords list\n",
    "not_stopwords = {'no', 'don', 'not'}\n",
    "new_stopwords = set([word for word in stopwords if word not in not_stopwords])\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "lm =  WordNetLemmatizer()\n",
    "number='0123456789'\n",
    "\n",
    "def no_space(sentence):\n",
    "    sentence = [x.replace(' ', '') for x in sentence]\n",
    "\n",
    "    return sentence\n",
    "\n",
    "#initial clean removes punctuation and sets all text to lower case\n",
    "def initial_clean(text):\n",
    "   \n",
    "    if text not in exception:\n",
    "        text = re.sub('(N)(\\d+)','',str(text))   # remove wells and equipments names\n",
    "\n",
    "    #remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub(\"(\\')\",'',str(text))\n",
    "    text = re.sub(r\"(\\W)\",r' ',str(text))\n",
    "    text = re.sub(r\" +\",r' ',str(text))\n",
    "    text = re.sub(\"\\/\",\" \",str(text))\n",
    "    text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+','',str(text))\n",
    "    text = re.sub('^[0-9 ]+', '', str(text))\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \",str(text))\n",
    "    nopunct = text.lower()\n",
    "    nonumb= \"\".join([char for char in nopunct if char not in number])\n",
    "   \n",
    "    return nonumb\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "nlp = spacy.load('en', disable = ['ner', 'textcat', 'similarity', 'merge_noun_chunks', 'merge_entities', 'tensorizer', 'parser', 'sbd', 'sentencizer'])\n",
    "\n",
    "\n",
    "def lemmatize_sentences(nlp_spacy, text_series):\n",
    "    lemmatized_sentences = []\n",
    "    for text in text_series.astype(str):\n",
    "        lemmas = []\n",
    "        for token in nlp_spacy(text):\n",
    "            lemmas.append(token.lemma_)\n",
    "        lemmatized_sentences.append(' '.join(lemmas))\n",
    "    return lemmatized_sentences\n",
    "\n",
    "#stems words, applies the spelling corrector and removes any stopwords\n",
    "def clean_data_stem(text):\n",
    "    #split review into words tokenized\n",
    "    tokens = re.split('\\W+',text) \n",
    "    #text = [ ps.stem(correction(word)) for word in tokens if word not in stopwords]\n",
    "    text = [ word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "def data_for_ngram(text):\n",
    "    new_text=\"\"\n",
    "    for word in text: #.split():\n",
    "        if word not in stopwords:\n",
    "            #new_text=new_text+\" \"+ps.stem(correction(word))\n",
    "            #new_text=new_text+\" \"+ps.stem(word)\n",
    "            new_text=new_text+\" \"+word\n",
    "            \n",
    "    return new_text\n",
    "\n",
    "def ngram_count(words, text):\n",
    "     \n",
    "    count = 0\n",
    "    for word in text:  \n",
    "        if word == words:\n",
    "            count +=1 \n",
    "    return count\n",
    "\n",
    "\n",
    "def ngram_list(text):\n",
    "\n",
    "    list_ngram=[]\n",
    "    bigrams=ngrams(text.split(),2)\n",
    "    for gram in bigrams:\n",
    "        newgram=\"\"\n",
    "        for word in gram:\n",
    "            newgram=newgram+word+\" \"\n",
    "        list_ngram.append(newgram[0:len(newgram)-1])\n",
    "    #trigrams=ngrams(text.split(),3)\n",
    "    #for gram in trigrams:\n",
    "    #    newgram=\"\"\n",
    "    #    for word in gram:\n",
    "    #        newgram=newgram+word+\" \"\n",
    "    #    list_ngram.append(newgram[0:len(newgram)-1])\n",
    "\n",
    "    return list_ngram\n",
    "\n",
    "\n",
    "\n",
    "def feature_selection(data):\n",
    "\n",
    "    #does the initial clean ready to find any double bonded words\n",
    "    data['Initial Clean']=data['SHADOW_COMMENT'].apply(lambda x: initial_clean(x))\n",
    "    data['Stem']=data['Initial Clean'].apply(lambda x: clean_data_stem(x))\n",
    "    data['ngram']=data['Initial Clean'].apply(lambda x: data_for_ngram(x))\n",
    "    data['grams']=data['ngram'].apply(lambda x: ngram_list(x))\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Opralog['clean_opra']=lemmatize_sentences(nlp,Opralog['clean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Opralog['clean_opra']=Opralog.clean_opra.apply(lambda x: clean_data_stem(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Opralog['ngram']=Opralog.clean_opra.apply(lambda x: data_for_ngram(x))\n",
    "Opralog=Opralog.loc[~Opralog['ngram'].str.contains('nan')]\n",
    "Opralog['grams']=Opralog.ngram.apply(lambda x: ngram_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Opralog['ngrams2']= Opralog.grams.apply(lambda x:  data_for_ngram(no_space(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Opralog_stim=Opralog[Opralog['ngram'].str.contains('stim')]  \n",
    "\n",
    "Opralog_stim #140 times\n",
    "\n",
    "# On the topic of stimming;\n",
    "# •\tTopic 3 looked like the best list of related words for stimming Dunbar wells \n",
    "# (3, '0.008*\"drain stimulation\" + 0.008*\"drain stim\" + 0.008*\"stimulation try\" + 0.008*\"route drain\"')\n",
    "# •\tNo distinct topic looks best suited to Alwyn wells. Would maybe expect “C104” or “test” to be relevant terms.\n",
    "\n",
    "# For changes to a wells flowing behaviour (not necessarily a start or shut-in);\n",
    "# •\tAll wells- Routed, Rerouted, swung, choke, choked, cut back, test, lined up to \n",
    "# •\tDunbar wells – Suction pressure\n",
    "# •\tDunbar & Islay – HP Mode, LP Mode\n",
    "# •\tA01 – BDV bypass, arrivals, PWRI\n",
    "\n",
    "# For wells about to be shut-in/started;\n",
    "# •\tCITHP, DHSV, equalise, equalised, remove gas cap, offload gas cap\n",
    "\n",
    "Opralog_c104=Opralog[Opralog['SHADOW_COMMENT'].str.contains('C104')]\n",
    "Opralog_c104 #82 times\n",
    "\n",
    "Opralog.LOGBOOK_NAME.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "df = []\n",
    "df = pd.DataFrame(columns=['explained_variance_ratio', 'components'])\n",
    "j=0\n",
    "                      \n",
    "for i in range(100, 1500, 100):\n",
    "    tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')\n",
    "    svd = TruncatedSVD(n_components=i)\n",
    "    transformed_x_train = tfidf_vec.fit_transform(Opralog[\"ngram\"])\n",
    "    x_train_svd = svd.fit_transform(transformed_x_train)\n",
    "    df.loc[j,'explained_variance_ratio'] = svd.explained_variance_ratio_.sum()\n",
    "    df.loc[j,'components'] = i\n",
    "    j=j+1\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from textblob import TextBlob\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')\n",
    "svd = TruncatedSVD(n_components=350)\n",
    "transformed_x_train = tfidf_vec.fit_transform(Opralog[\"ngram\"])\n",
    "lsa_topic_matrix= svd.fit_transform(transformed_x_train)\n",
    "    \n",
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
    "\n",
    "top_n_words_lsa = get_top_n_words(10, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "    # each review item becomes a serries of words\n",
    "    # so this becomes a list of lists\n",
    "documents = list(Opralog.clean_opra)\n",
    "    \n",
    "    # build vocabulary and train model\n",
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        size=150,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10)\n",
    "model.train(documents, total_examples=len(documents), epochs=10)\n",
    "\n",
    "documents\n",
    "        \n",
    "    \n",
    "redundant_terms = [ 'baker drawer', 'drawer', 'a annulus', 'b annulus', 'c annulus', 'annulus a', 'annulus b', 'annulus c',\n",
    "                   'a annuli', 'b annuli', 'c annuli', 'annuli a', 'annuli b', 'annuli c', 'annuli', 'alarm', 'high alarm',\n",
    "                   'annulus', 'Words','bleed off', 'top up', ' a ', ' b ', ' c ',   'bleed', 'update', 'bled', 'bleeding', 'thermal', 'corrosion coupon',\n",
    "                   'wims', 'mls/min', 'injection', 'inhibitor', 'leak off', 'leak rate', 'gas lift', 'scale pump']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = ['stim']\n",
    "print(\"Most similar to {0}\".format(w1),model.wv.most_similar(positive=w1,topn=10))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top words on unknown\n",
    "list_wells=Opralog.WELLNAME.unique()\n",
    "#list_years=Opralog.Year.unique()\n",
    "list_wells=list_wells.tolist()\n",
    "vec = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in list_years:\n",
    "    df2 = pd.DataFrame(vec.fit_transform(Opralog.loc[Opralog.Year==i,\"ngram\"]).toarray(), \n",
    "                      columns=vec.get_feature_names())\n",
    "    print(i)\n",
    "    print(df2.sum().sort_values(ascending=False).head(20)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in list_wells:\n",
    "    df2 = pd.DataFrame(vec.fit_transform(Opralog.loc[Opralog.WELLNAME==i,\"ngram\"]).toarray(), \n",
    "                      columns=vec.get_feature_names())\n",
    "    print(i)\n",
    "    print(df2.sum().sort_values(ascending=False).head(20)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vec.fit_transform(Opralog[\"ngram\"]).toarray(), \n",
    "                      columns=vec.get_feature_names())\n",
    "\n",
    "print(df.sum().sort_values(ascending=False).head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "df2 = pd.DataFrame(vec.fit_transform(Opralog[\"ngrams2\"]).toarray(), \n",
    "                  columns=vec.get_feature_names())\n",
    "print(df2.sum().sort_values(ascending=False).head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Opralog_Dunbar=Opralog.loc[Opralog.LOGBOOK_NAME.isin(['Dunbar CCR Logbook','Dunbar Welltech Logbook'])]\n",
    "Opralog_Alwyn=Opralog.loc[Opralog.LOGBOOK_NAME.isin(['Alwyn North CCR Logbook','NAA Operators Logbook'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import word2vec\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "#text_data = []\n",
    "#with open('dataset.csv') as f:\n",
    "#    for line in f:\n",
    "#        tokens = prepare_text_for_lda(line)\n",
    "#        if random.random() > .99:\n",
    "#            print(tokens)\n",
    "#            text_data.append(tokens)\n",
    "           \n",
    "    #clean_opra\n",
    "    #grams\n",
    "\n",
    "dictionary = corpora.Dictionary(Opralog_Dunbar[\"grams\"])\n",
    "corpus = [dictionary.doc2bow(text) for text in Opralog_Dunbar[\"grams\"]]\n",
    "\n",
    "dictionary_alwyn = corpora.Dictionary(Opralog_Alwyn[\"grams\"])\n",
    "corpus_alwyn = [dictionary_alwyn.doc2bow(text) for text in Opralog_Alwyn[\"grams\"]]\n",
    "\n",
    "\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')\n",
    "\n",
    "pickle.dump(corpus_alwyn, open('corpus_alwyn.pkl', 'wb'))\n",
    "dictionary_alwyn.save('dictionary_alwyn.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUNBAR TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS =6\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('topic_model_Dunbar.gensim')\n",
    "topics = ldamodel.print_topics(num_words=3)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('topic_model_Dunbar.gensim')\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)           \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALWYN TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 6\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus_alwyn, num_topics = NUM_TOPICS, id2word=dictionary_alwyn, passes=15)\n",
    "ldamodel.save('topic_model_Alwyn.gensim')\n",
    "topics = ldamodel.print_topics(num_words=3)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import pyLDAvis.gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load('dictionary_alwyn.gensim')\n",
    "corpus = pickle.load(open('corpus_alwyn.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('topic_model_Alwyn.gensim')\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#APPLY TOPIC MODELLING TO NEW DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "documents=Opralog.grams\n",
    "\n",
    "#for top in lda.print_topics():\n",
    "#    print(top\n",
    "          \n",
    "print(lda) # LdaModel(num_terms=2173, num_topics=5, decay=0.5, chunksize=2000)\n",
    "lda[corpus] #<gensim.interfaces.TransformedCorpus at 0x7fbda79a05c0>\n",
    "enumerate(lda_corpus)\n",
    "documents[2]\n",
    "lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "## Libraries to download\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "# Assinging the topics to the document in corpus\n",
    "lda_corpus = ldamodel[corpus] # <gensim.interfaces.TransformedCorpus at 0x7fbda79a06d8>\n",
    "\n",
    "# Find the threshold, let's set the threshold to be 1/#clusters,\n",
    "# To prove that the threshold is sane, we average the sum of all probabilities:\n",
    "scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                     for topic in [doc for doc in lda_corpus]]))\n",
    "\n",
    "threshold = sum(scores)/len(scores)\n",
    "print(threshold)\n",
    "\n",
    "cluster1 = [j for i,j in zip(lda_corpus,documents) if i[0][1] > threshold]\n",
    "cluster2 = [j for i,j in zip(lda_corpus,documents) if i[1][1] > threshold]\n",
    "cluster3 = [j for i,j in zip(lda_corpus,documents) if i[2][1] > threshold]\n",
    "\n",
    "print(cluster3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
