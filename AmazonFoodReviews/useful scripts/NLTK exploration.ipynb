{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all relevent libraries to analyse the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This line makes sure that our graphs are rendered within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set options to display all columns\n",
    "# pd.set_option(\"display.max_columns\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>n_distinct_words</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-04-27</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>40</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-09-07</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>67</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-06-13</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-10-21</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "0                     1                       1      5 2011-04-27   \n",
       "1                     0                       0      1 2012-09-07   \n",
       "2                     1                       1      4 2008-08-18   \n",
       "3                     3                       3      2 2011-06-13   \n",
       "4                     0                       0      5 2012-10-21   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "   n_distinct_words  n_words  \n",
       "0                40       48  \n",
       "1                27       32  \n",
       "2                67       93  \n",
       "3                36       41  \n",
       "4                21       27  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "# Display the size of the dataset and first couple of rows to see what it looks like\n",
    "data=pd.read_csv('Reviews.csv')\n",
    "\n",
    "def get_word_counts(text):\n",
    "    tokenized_word=tokenizer.tokenize(text)\n",
    "    fdist = FreqDist(tokenized_word)\n",
    "    return [fdist.B(), fdist.N()]\n",
    "\n",
    "print(data.shape)\n",
    "text_attributes = pd.DataFrame(data[\"Text\"].map(get_word_counts), data.index)[\"Text\"].apply(pd.Series)\n",
    "text_attributes.columns = [\"n_distinct_words\", \"n_words\"]\n",
    "data = pd.concat([data[:], text_attributes[:]], axis=1)\n",
    "# data[\"n_distinct_words_and_n_words\"] = data[\"Text\"].map(get_word_counts)\n",
    "data[\"Time\"] = data[\"Time\"].map(lambda timestamp: pd.Timestamp(timestamp, unit=\"s\"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from nltk) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rmartinez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rmartinez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rmartinez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rmartinez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568454"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'ProductId',\n",
       " 'UserId',\n",
       " 'ProfileName',\n",
       " 'HelpfulnessNumerator',\n",
       " 'HelpfulnessDenominator',\n",
       " 'Score',\n",
       " 'Time',\n",
       " 'Summary',\n",
       " 'Text']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.655121333728353"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ProductId.value_counts()\n",
    "data.ProductId.nunique() \n",
    "\n",
    "568454/74258\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n"
     ]
    }
   ],
   "source": [
    "Text_test = data.loc[0,'Text']\n",
    "print(Text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "Text_test.translate(dict.fromkeys(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " 'bought',\n",
       " 'several',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Vitality',\n",
       " 'canned',\n",
       " 'dog',\n",
       " 'food',\n",
       " 'products',\n",
       " 'and',\n",
       " 'have',\n",
       " 'found',\n",
       " 'them',\n",
       " 'all',\n",
       " 'to',\n",
       " 'be',\n",
       " 'of',\n",
       " 'good',\n",
       " 'quality',\n",
       " 'The',\n",
       " 'product',\n",
       " 'looks',\n",
       " 'more',\n",
       " 'like',\n",
       " 'a',\n",
       " 'stew',\n",
       " 'than',\n",
       " 'a',\n",
       " 'processed',\n",
       " 'meat',\n",
       " 'and',\n",
       " 'it',\n",
       " 'smells',\n",
       " 'better',\n",
       " 'My',\n",
       " 'Labrador',\n",
       " 'is',\n",
       " 'finicky',\n",
       " 'and',\n",
       " 'she',\n",
       " 'appreciates',\n",
       " 'this',\n",
       " 'product',\n",
       " 'better',\n",
       " 'than',\n",
       " 'most']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(Text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', 'My', 'Labrador', 'is', 'finicky', 'and', 'she', 'appreciates', 'this', 'product', 'better', 'than', 'most']\n",
      "{'own', 'myself', 'because', 'very', \"shan't\", 'about', 'don', 'are', 'which', 'wasn', 'or', 'am', 'from', 'over', 'just', 've', 'their', 'all', \"didn't\", 'hers', 'then', \"needn't\", 'll', 'but', 're', \"that'll\", 'some', \"isn't\", \"you're\", 'yours', 'it', 't', 'haven', 'whom', 'after', 'isn', 'under', 'shouldn', 'yourself', 'have', 'who', 'those', 'being', 'into', \"mustn't\", 'them', \"hasn't\", 'down', 'y', 'does', 'hadn', 'ain', 'ourselves', 'not', 'himself', 'once', 'more', \"don't\", 'further', \"wouldn't\", \"you'd\", \"you've\", 'doesn', \"aren't\", \"shouldn't\", 'ma', 'other', 'only', \"it's\", 'of', 'these', 'will', 'hasn', 'doing', 'off', 's', 'here', 'now', 'to', 'my', 'didn', \"wasn't\", \"haven't\", 'having', 'too', 'our', 'so', 'during', 'between', 'by', 'do', 'for', 'herself', 'be', 'shan', 'can', 'needn', 'than', 'no', 'o', 'if', 'we', 'did', 'its', 'theirs', 'his', 'before', \"she's\", 'up', 'few', 'you', 'should', 'most', 'aren', 'the', 'same', 'through', 'won', \"couldn't\", 'there', 'me', 'm', 'couldn', 'in', 'mustn', 'when', \"doesn't\", 'themselves', 'below', 'been', 'him', 'why', 'on', 'above', 'nor', 'what', \"mightn't\", 'each', 'mightn', 'while', 'a', 'd', 'against', 'at', 'where', 'weren', 'this', 'that', 'itself', 'out', \"hadn't\", 'he', 'wouldn', 'is', \"you'll\", 'your', 'i', 'with', 'such', 'had', 'again', 'they', 'any', \"won't\", 'an', 'both', 'were', 'ours', 'yourselves', 'she', 'how', 'her', 'until', \"should've\", 'as', 'was', 'has', 'and', \"weren't\"}\n",
      "40\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# tokenized_word=word_tokenize(Text_test)\n",
    "tokenized_word=tokenizer.tokenize(Text_test)\n",
    "fdist = FreqDist(tokenized_word)\n",
    "stop_words=set(stopwords.words(\"english\")) # print(stop_words)\n",
    "\n",
    "print(tokenized_word)\n",
    "print(stop_words)\n",
    "print(fdist.B())\n",
    "print(fdist.N())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:1\n",
      "bought:1\n",
      "several:1\n",
      "Vitality:1\n",
      "canned:1\n",
      "dog:1\n",
      "food:1\n",
      "products:1\n",
      "found:1\n",
      "good:1\n",
      "quality:1\n",
      "The:1\n",
      "product:2\n",
      "looks:1\n",
      "like:1\n",
      "stew:1\n",
      "processed:1\n",
      "meat:1\n",
      "smells:1\n",
      "better:2\n",
      "My:1\n",
      "Labrador:1\n",
      "finicky:1\n",
      "appreciates:1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sr= stopwords.words('english')\n",
    "clean_tokens = tokenized_word[:]\n",
    "for token in tokenized_word:\n",
    "    if token in stopwords.words('english'):\n",
    "        \n",
    "        clean_tokens.remove(token)\n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "\n",
    "for key,val in freq.items():\n",
    "    print(str(key) + ':' + str(val))\n",
    "freq.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 3), ('have', 2)]\n",
      "[('product', 2), ('better', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(fdist.most_common(2))\n",
    "print(list(filter(lambda t: t[1] > 1,freq.most_common(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', 'My', 'Labrador', 'is', 'finicky', 'and', 'she', 'appreciates', 'this', 'product', 'better', 'than', 'most']\n",
      "Filterd Sentence: ['I', 'bought', 'several', 'Vitality', 'canned', 'dog', 'food', 'products', 'found', 'good', 'quality', 'The', 'product', 'looks', 'like', 'stew', 'processed', 'meat', 'smells', 'better', 'My', 'Labrador', 'finicky', 'appreciates', 'product', 'better']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized Sentence:\",tokenized_word)\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['I', 'bought', 'several', 'Vitality', 'canned', 'dog', 'food', 'products', 'found', 'good', 'quality', 'The', 'product', 'looks', 'like', 'stew', 'processed', 'meat', 'smells', 'better', 'My', 'Labrador', 'finicky', 'appreciates', 'product', 'better']\n",
      "Stemmed Sentence: ['I', 'bought', 'sever', 'vital', 'can', 'dog', 'food', 'product', 'found', 'good', 'qualiti', 'the', 'product', 'look', 'like', 'stew', 'process', 'meat', 'smell', 'better', 'My', 'labrador', 'finicki', 'appreci', 'product', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "#Lexicon Normalization\n",
    "#performing stemming and Lemmatization\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', '.', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', '.', 'My', 'Labrador', 'is', 'finicky', 'and', 'she', 'appreciates', 'this', 'product', 'better', 'than', 'most', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('bought', 'VBN'),\n",
       " ('several', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Vitality', 'NNP'),\n",
       " ('canned', 'VBD'),\n",
       " ('dog', 'RP'),\n",
       " ('food', 'NN'),\n",
       " ('products', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('have', 'VBP'),\n",
       " ('found', 'VBN'),\n",
       " ('them', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('of', 'IN'),\n",
       " ('good', 'JJ'),\n",
       " ('quality', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('product', 'NN'),\n",
       " ('looks', 'VBZ'),\n",
       " ('more', 'RBR'),\n",
       " ('like', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('stew', 'NN'),\n",
       " ('than', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('processed', 'JJ'),\n",
       " ('meat', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " ('smells', 'VBZ'),\n",
       " ('better', 'RBR'),\n",
       " ('.', '.'),\n",
       " ('My', 'PRP$'),\n",
       " ('Labrador', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('finicky', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('she', 'PRP'),\n",
       " ('appreciates', 'VBZ'),\n",
       " ('this', 'DT'),\n",
       " ('product', 'NN'),\n",
       " ('better', 'JJR'),\n",
       " ('than', 'IN'),\n",
       " ('most', 'JJS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=nltk.word_tokenize(Text_test)\n",
    "print(tokens)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (0.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\rmartinez\\appdata\\roaming\\python\\python37\\site-packages (from spacy) (1.15.4)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (2.6.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (2.19.1)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (7.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.23)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\rmartinez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.26.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rmartinez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "string.punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#from stemming.porter2 import stem\n",
    "\n",
    "#remove words that are in NLTK stopwords list\n",
    "not_stopwords = {'no', 'don', 'not'}\n",
    "new_stopwords = set([word for word in stopwords if word not in not_stopwords])\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "lm =  WordNetLemmatizer()\n",
    "number='0123456789'\n",
    "\n",
    "def no_space(sentence):\n",
    "    sentence = [x.replace(' ', '') for x in sentence]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial clean removes punctuation and sets all text to lower case\n",
    "def initial_clean(text):\n",
    "   \n",
    "    #remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub(\"(\\')\",'',str(text))\n",
    "    text = re.sub(r\"(\\W)\",r' ',str(text))\n",
    "    text = re.sub(r\" +\",r' ',str(text))\n",
    "    text = re.sub(\"\\/\",\" \",str(text))\n",
    "    text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+','',str(text))\n",
    "    text = re.sub('^[0-9 ]+', '', str(text))\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \",str(text))\n",
    "    nopunct = text.lower()\n",
    "    nonumb= \"\".join([char for char in nopunct if char not in number])\n",
    "   \n",
    "    return nonumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-7b6c422ca890>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'textcat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'similarity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'merge_noun_chunks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'merge_entities'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tensorizer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sbd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentencizer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "nlp = spacy.load('en', disable = ['ner', 'textcat', 'similarity', 'merge_noun_chunks', 'merge_entities', 'tensorizer', 'parser', 'sbd', 'sentencizer'])\n",
    "\n",
    "\n",
    "def lemmatize_sentences(nlp_spacy, text_series):\n",
    "    lemmatized_sentences = []\n",
    "    for text in text_series.astype(str):\n",
    "        lemmas = []\n",
    "        for token in nlp_spacy(text):\n",
    "            lemmas.append(token.lemma_)\n",
    "        lemmatized_sentences.append(' '.join(lemmas))\n",
    "    return lemmatized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rmartinez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-5b8eaa97d7c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'textcat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'similarity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'merge_noun_chunks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'merge_entities'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tensorizer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sbd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentencizer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "nlp = spacy.load('en', disable = ['ner', 'textcat', 'similarity', 'merge_noun_chunks', 'merge_entities', 'tensorizer', 'parser', 'sbd', 'sentencizer'])\n",
    "\n",
    "\n",
    "def lemmatize_sentences(nlp_spacy, text_series):\n",
    "    lemmatized_sentences = []\n",
    "    for text in text_series.astype(str):\n",
    "        lemmas = []\n",
    "        for token in nlp_spacy(text):\n",
    "            lemmas.append(token.lemma_)\n",
    "        lemmatized_sentences.append(' '.join(lemmas))\n",
    "    return lemmatized_sentences\n",
    "\n",
    "#stems words, applies the spelling corrector and removes any stopwords\n",
    "def clean_data_stem(text):\n",
    "    #split review into words tokenized\n",
    "    tokens = re.split('\\W+',text) \n",
    "    #text = [ ps.stem(correction(word)) for word in tokens if word not in stopwords]\n",
    "    text = [ word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "def data_for_ngram(text):\n",
    "    new_text=\"\"\n",
    "    for word in text: #.split():\n",
    "        if word not in stopwords:\n",
    "            #new_text=new_text+\" \"+ps.stem(correction(word))\n",
    "            #new_text=new_text+\" \"+ps.stem(word)\n",
    "            new_text=new_text+\" \"+word\n",
    "            \n",
    "    return new_text\n",
    "\n",
    "def ngram_count(words, text):\n",
    "     \n",
    "    count = 0\n",
    "    for word in text:  \n",
    "        if word == words:\n",
    "            count +=1 \n",
    "    return count\n",
    "\n",
    "\n",
    "def ngram_list(text):\n",
    "\n",
    "    list_ngram=[]\n",
    "    bigrams=ngrams(text.split(),2)\n",
    "    for gram in bigrams:\n",
    "        newgram=\"\"\n",
    "        for word in gram:\n",
    "            newgram=newgram+word+\" \"\n",
    "        list_ngram.append(newgram[0:len(newgram)-1])\n",
    "    #trigrams=ngrams(text.split(),3)\n",
    "    #for gram in trigrams:\n",
    "    #    newgram=\"\"\n",
    "    #    for word in gram:\n",
    "    #        newgram=newgram+word+\" \"\n",
    "    #    list_ngram.append(newgram[0:len(newgram)-1])\n",
    "\n",
    "    return list_ngram\n",
    "\n",
    "\n",
    "\n",
    "def feature_selection(data):\n",
    "\n",
    "    #does the initial clean ready to find any double bonded words\n",
    "    data['Initial Clean']=data['text'].apply(lambda x: initial_clean(x))\n",
    "    data['Stem']=data['Initial Clean'].apply(lambda x: clean_data_stem(x))\n",
    "    data['ngram']=data['Initial Clean'].apply(lambda x: data_for_ngram(x))\n",
    "    data['grams']=data['ngram'].apply(lambda x: ngram_list(x))\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>n_distinct_words</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>42</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "      <td>57</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId              ProfileName  \\\n",
       "568449  568450  B001EO7N10  A28KG5XORO54AY         Lettie D. Carter   \n",
       "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                R. Sawyer   \n",
       "568451  568452  B004I613EE  A121AA1GQV751Z            pksd \"pk_007\"   \n",
       "568452  568453  B004I613EE   A3IBEVCTXKNOH  Kathy A. Welch \"katwel\"   \n",
       "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                 srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "568449                     0                       0      5  1299628800   \n",
       "568450                     0                       0      2  1331251200   \n",
       "568451                     2                       2      5  1329782400   \n",
       "568452                     1                       1      5  1331596800   \n",
       "568453                     0                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  n_distinct_words  \\\n",
       "568449  Great for sesame chicken..this is a good if no...                28   \n",
       "568450  I'm disappointed with the flavor. The chocolat...                42   \n",
       "568451  These stars are small, so you can give 10-15 o...                57   \n",
       "568452  These are the BEST treats for training and rew...                32   \n",
       "568453  I am very satisfied ,product is as advertised,...                19   \n",
       "\n",
       "        n_words  \n",
       "568449       29  \n",
       "568450       49  \n",
       "568451       67  \n",
       "568452       35  \n",
       "568453       21  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
